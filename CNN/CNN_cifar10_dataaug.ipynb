{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, regularizers, optimizers\n",
    "import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "VERBOSE = 1\n",
    "OPTIM = tf.keras.optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image characteristics (CIFAR_!= is a set of 60k images 32x32 pixels on 3 channels)\n",
    "IMG_CHANNELS, IMG_ROWS, IMG_COLS = 3, 32, 32 #input image dimensions\n",
    "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, IMG_CHANNELS)\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    #load data\n",
    "    (X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()\n",
    "    #cast\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    #Standardization\n",
    "    mean = np.mean(X_train, axis =(0,1,2,3))\n",
    "    std = np.std(X_train, axis =(0,1,2,3))\n",
    "    eps = 1e-7\n",
    "    X_train = (X_train - mean) / (std + eps)\n",
    "    X_test = (X_test - mean) / (std + eps)\n",
    "    #convert class vectors to binary class matrices\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "X_train, X_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's define a convnet with the following modules:\n",
    "# 1st --> (CONV + CONV + MaxPool + DropOut)\n",
    "# 2nd --> (CONV + CONV + MaxPool + DropOut)\n",
    "# 3rd --> (CONV + CONV + MaxPool + DropOut)\n",
    "# dense --> (FLATTEN + SOFTMAX classifier)\n",
    "\n",
    "def build_model(input_shape, classes):\n",
    "    model = models.Sequential()\n",
    "    #1st module\n",
    "    model.add(layers.Conv2D(32, (3,3), padding = 'same', activation = 'relu', input_shape = input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(32, (3,3), padding = 'same', activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    #2nd module\n",
    "    model.add(layers.Conv2D(64, (3,3), padding = 'same', activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(64, (3,3), padding = 'same', activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    #3rd module\n",
    "    model.add(layers.Conv2D(128, (3,3), padding = 'same', activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2D(128, (3,3), padding = 'same', activation = 'relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    #dense\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(classes, activation = \"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorBoard\n",
    "LOG_DIR = \"logs/CNN_cifar10_dataaug/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir = LOG_DIR)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#initialize the optimizer and the model\n",
    "model = build_model(input_shape = INPUT_SHAPE, classes = NUM_CLASSES)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = OPTIM, metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range = 30,\n",
    "        width_shift_range = 0.2,\n",
    "        height_shift_range = 0.2,\n",
    "        horizontal_flip = True)\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-8bb4d9307681>:2: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "  1/782 [..............................] - ETA: 0s - loss: 4.6616 - accuracy: 0.1250WARNING:tensorflow:From C:\\Users\\ER180124\\.conda\\envs\\PrognosticEnv\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "782/782 [==============================] - 368s 470ms/step - loss: 2.0958 - accuracy: 0.3638 - val_loss: 1.5703 - val_accuracy: 0.4833\n",
      "Epoch 2/50\n",
      "782/782 [==============================] - 365s 467ms/step - loss: 1.5668 - accuracy: 0.4979 - val_loss: 1.4752 - val_accuracy: 0.5361\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 345s 442ms/step - loss: 1.3826 - accuracy: 0.5558 - val_loss: 1.1879 - val_accuracy: 0.6235\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 341s 437ms/step - loss: 1.2444 - accuracy: 0.5888 - val_loss: 1.3703 - val_accuracy: 0.6040\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 341s 436ms/step - loss: 1.1324 - accuracy: 0.6188 - val_loss: 1.0091 - val_accuracy: 0.6656\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 341s 436ms/step - loss: 1.0451 - accuracy: 0.6440 - val_loss: 0.8881 - val_accuracy: 0.7034\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 342s 437ms/step - loss: 0.9835 - accuracy: 0.6637 - val_loss: 0.8095 - val_accuracy: 0.7300\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 341s 437ms/step - loss: 0.9340 - accuracy: 0.6798 - val_loss: 0.8327 - val_accuracy: 0.7217\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 341s 436ms/step - loss: 0.8857 - accuracy: 0.6937 - val_loss: 0.8985 - val_accuracy: 0.7159\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 342s 437ms/step - loss: 0.8547 - accuracy: 0.7051 - val_loss: 0.8892 - val_accuracy: 0.7184\n",
      "Epoch 11/50\n",
      "782/782 [==============================] - 344s 440ms/step - loss: 0.8273 - accuracy: 0.7126 - val_loss: 0.6980 - val_accuracy: 0.7698\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 354s 453ms/step - loss: 0.8006 - accuracy: 0.7226 - val_loss: 0.7176 - val_accuracy: 0.7693\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 395s 505ms/step - loss: 0.7850 - accuracy: 0.7295 - val_loss: 0.6331 - val_accuracy: 0.7902\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 415s 530ms/step - loss: 0.7566 - accuracy: 0.7364 - val_loss: 0.7093 - val_accuracy: 0.7691\n",
      "Epoch 15/50\n",
      "782/782 [==============================] - 405s 519ms/step - loss: 0.7488 - accuracy: 0.7414 - val_loss: 0.6860 - val_accuracy: 0.7796\n",
      "Epoch 16/50\n",
      "570/782 [====================>.........] - ETA: 1:52 - loss: 0.7426 - accuracy: 0.7437"
     ]
    }
   ],
   "source": [
    "#fit the model and evaluate\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size = BATCH_SIZE), epochs = EPOCHS, verbose = VERBOSE, validation_data = (X_test, y_test), callbacks = callbacks)\n",
    "score = model.evaluate(X_test, y_test, batch_size = BATCH_SIZE, verbose = VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to disk\n",
    "model_json = model.to_json()\n",
    "with open('cifar10_model.json','w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('cifar10_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting without training each time\n",
    "from tensorflow.keras.models import model_from_json\n",
    "model = model_from_json(open('cifar10_model.json').read())\n",
    "model.load_weights('cifar10_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load images to be predicted\n",
    "from imageio import imread\n",
    "from skimage.transform import resize\n",
    "img_names = ['cat-standing.jpg','dog.jpg']\n",
    "imgs = [resize(imread(img_name), (32,32)).astype(\"float32\") for img_name in img_names]\n",
    "imgs = np.array(imgs) / 255\n",
    "print(\"imgs.shape: \",imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = OPTIM, metrics = ['accuracy'])\n",
    "#predict\n",
    "predictions =np.argmax(model.predict(imgs), axis=-1)\n",
    "print(\"predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
